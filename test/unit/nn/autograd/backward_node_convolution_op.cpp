#include "nn/autograd/backward_node_convolution_op.h"
#include <gtest/gtest.h>
#include "nn/autograd/autograd.h"
#include "nn/debug/debug_utils.h"
#include "nn/modules/conv2d.h"
#include "nn/operations/common_types.h"
#include "nn/operations/losses.h"
#include "nn/operations/matrix.h"
#include "nn/operations/tensor_operations.h"
#include "nn/tensor/tensor.h"
#include "nn/tensor/tensor_creator.h"

using namespace toytorch;

class Conv2dBackwardTest : public testing::Test {

 protected:
  constexpr static int input_h = 5;
  constexpr static int input_w = 5;

  constexpr static int batch_size = 2;
  constexpr static int in_channels = 8;
  constexpr static int out_channels = 6;
  constexpr static int kernel_h = 2;
  constexpr static int kernel_w = 3;

  Conv2dBackwardTest()
      : conv_(in_channels, out_channels, {kernel_h, kernel_w}) {

    conv_.debug_set_weights(Tensor(
        {out_channels, in_channels, kernel_h, kernel_w},
        {0.1225,  -0.1128, 0.1288,  0.0126,  -0.1326, 0.1165,  -0.0090, -0.0862,
         0.0242,  0.0481,  0.0758,  0.0378,  0.0852,  -0.0318, 0.0970,  0.1307,
         0.0110,  0.0643,  0.0071,  -0.0285, 0.1433,  0.1063,  -0.1262, -0.0977,
         -0.0563, 0.0526,  0.0842,  0.0435,  0.0589,  0.0675,  -0.0649, -0.1210,
         -0.1074, -0.0126, -0.0080, -0.0683, 0.0133,  -0.0362, -0.0960, 0.1437,
         -0.0535, -0.0656, -0.0483, -0.0778, -0.0088, -0.0181, -0.0297, -0.1092,
         0.0315,  0.0978,  0.0991,  -0.0981, 0.1344,  -0.0795, -0.0084, -0.0819,
         0.0687,  0.0180,  -0.0680, 0.0249,  0.1054,  -0.0506, -0.0511, -0.1323,
         -0.0385, -0.1412, 0.0021,  -0.0188, 0.1230,  0.0660,  0.1373,  0.1226,
         0.0673,  0.0045,  -0.0611, -0.0307, 0.1406,  -0.0265, 0.0715,  0.0553,
         -0.0595, 0.0463,  -0.0643, 0.0406,  -0.0551, 0.0865,  0.1051,  0.0348,
         -0.0815, -0.0998, -0.0756, -0.0450, 0.0959,  0.1034,  0.0899,  0.1338,
         0.0234,  -0.0714, 0.1014,  -0.1121, 0.0605,  0.0004,  0.0406,  -0.0169,
         -0.0002, -0.0170, -0.0152, -0.1014, -0.0635, 0.1270,  -0.0017, 0.1053,
         0.0885,  -0.1022, -0.1175, -0.0196, -0.0114, -0.0823, 0.1153,  -0.1209,
         -0.0718, 0.1387,  0.0184,  0.0057,  0.0639,  -0.0119, 0.0417,  0.0604,
         0.0994,  0.0868,  0.0846,  -0.0663, -0.0935, 0.0129,  0.0143,  0.1434,
         -0.0252, -0.1243, 0.0196,  0.0263,  -0.0087, -0.0002, -0.0157, -0.0773,
         -0.1302, -0.0365, -0.1093, 0.0418,  -0.0762, -0.1094, 0.1126,  -0.0426,
         0.1073,  -0.0002, 0.1426,  -0.0653, 0.0732,  -0.0885, 0.1068,  -0.0784,
         -0.1136, -0.1249, 0.0814,  0.0875,  -0.0498, 0.0969,  -0.0777, -0.0563,
         -0.1047, 0.1269,  -0.0807, 0.0113,  -0.0694, -0.0079, -0.0016, 0.0121,
         -0.0447, 0.0348,  -0.1158, 0.0926,  0.1188,  0.0861,  0.1157,  -0.0794,
         -0.1039, -0.0504, -0.1427, -0.0588, 0.0430,  -0.0961, 0.0492,  -0.0459,
         0.0062,  -0.1042, 0.1122,  0.0956,  0.1154,  -0.1053, 0.1091,  0.0633,
         -0.1001, 0.0545,  -0.0531, -0.0281, -0.1425, -0.0399, 0.1252,  -0.0016,
         0.1140,  0.1269,  0.1150,  0.0423,  0.0439,  -0.1393, 0.0428,  0.0236,
         -0.1085, -0.0964, 0.1351,  -0.0933, 0.0117,  -0.0793, 0.0546,  0.0906,
         -0.1354, 0.0311,  -0.1149, 0.1266,  0.1320,  -0.1188, -0.0053, 0.0429,
         -0.0785, -0.0007, 0.0460,  0.0628,  0.1069,  0.1183,  0.0703,  0.0330,
         -0.0483, 0.0408,  0.1346,  0.0371,  -0.1273, -0.1250, 0.0839,  0.1338,
         0.1158,  -0.0858, 0.0834,  -0.0060, 0.0671,  0.0875,  0.0440,  0.0326,
         0.0736,  0.0961,  0.0213,  -0.0773, 0.1442,  -0.0673, -0.1277, 0.0913,
         0.1052,  0.0119,  -0.0021, -0.1051, 0.0071,  0.0991,  -0.0926, 0.0576,
         -0.1112, -0.0249, 0.1207,  -0.0045, -0.0157, 0.1008,  -0.0073, -0.1239,
         -0.1154, 0.0489,  -0.0790, -0.0802, -0.0507, 0.0205,  0.0253,  0.0259},
        true));

    conv_.debug_set_bias(Tensor(
        {6, 1, 1}, {-0.0309, -0.0957, 0.1231, 0.0454, 0.1217, 0.0560}, true));

    input_ = ones({batch_size, in_channels, input_h, input_w}, true);
  }

  void run_forward_backward() {
    Tensor result = conv_.forward(input_).sum();
    result.backward();
  }

  Tensor input_;
  Conv2d conv_;
};

// See unit/nn/modules/conv2d.cpp
TEST_F(Conv2dBackwardTest, Backward) {

  // std::cout << debug::print_backward_graph(result) << std::endl;
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 24), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 24), 1e-6, 1e-4));

  // input_.grad()->print();
  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.0051,  -0.1812, 0.2856,  0.2805,  0.4668,  -0.0180, -0.2301,
              -0.0656, -0.0476, 0.1645,  -0.0180, -0.2301, -0.0656, -0.0476,
              0.1645,  -0.0180, -0.2301, -0.0656, -0.0476, 0.1645,  -0.0231,
              -0.0489, -0.3512, -0.3281, -0.3023, 0.3288,  0.2983,  0.5140,
              0.1852,  0.2157,  0.3464,  0.4814,  0.5590,  0.2126,  0.0776,
              0.3464,  0.4814,  0.5590,  0.2126,  0.0776,  0.3464,  0.4814,
              0.5590,  0.2126,  0.0776,  0.0176,  0.1831,  0.0450,  0.0274,
              -0.1381, 0.1249,  0.1286,  0.4488,  0.3239,  0.3202,  0.1812,
              0.3199,  0.5591,  0.3779,  0.2392,  0.1812,  0.3199,  0.5591,
              0.3779,  0.2392,  0.1812,  0.3199,  0.5591,  0.3779,  0.2392,
              0.0563,  0.1913,  0.1103,  0.0540,  -0.0810, 0.1094,  0.0950,
              0.4882,  0.3788,  0.3932,  0.0897,  0.0391,  0.3949,  0.3052,
              0.3558,  0.0897,  0.0391,  0.3949,  0.3052,  0.3558,  0.0897,
              0.0391,  0.3949,  0.3052,  0.3558,  -0.0197, -0.0559, -0.0933,
              -0.0736, -0.0374, -0.1688, 0.0694,  0.1632,  0.3320,  0.0938,
              -0.3374, 0.1136,  0.2484,  0.5858,  0.1348,  -0.3374, 0.1136,
              0.2484,  0.5858,  0.1348,  -0.3374, 0.1136,  0.2484,  0.5858,
              0.1348,  -0.1686, 0.0442,  0.0852,  0.2538,  0.0410,  0.0087,
              0.1637,  -0.1951, -0.2038, -0.3588, 0.1702,  0.2275,  -0.0106,
              -0.1808, -0.2381, 0.1702,  0.2275,  -0.0106, -0.1808, -0.2381,
              0.1702,  0.2275,  -0.0106, -0.1808, -0.2381, 0.1615,  0.0638,
              0.1845,  0.0230,  0.1207,  0.0998,  0.2311,  0.3576,  0.2578,
              0.1265,  0.2613,  -0.0654, -0.2308, -0.4921, -0.1654, 0.2613,
              -0.0654, -0.2308, -0.4921, -0.1654, 0.2613,  -0.0654, -0.2308,
              -0.4921, -0.1654, 0.1615,  -0.2965, -0.5884, -0.7499, -0.2919,
              -0.2800, -0.4527, -0.2751, 0.0049,  0.1776,  -0.1522, -0.1356,
              0.0023,  0.1545,  0.1379,  -0.1522, -0.1356, 0.0023,  0.1545,
              0.1379,  -0.1522, -0.1356, 0.0023,  0.1545,  0.1379,  0.1278,
              0.3171,  0.2774,  0.1496,  -0.0397, 0.0051,  -0.1812, 0.2856,
              0.2805,  0.4668,  -0.0180, -0.2301, -0.0656, -0.0476, 0.1645,
              -0.0180, -0.2301, -0.0656, -0.0476, 0.1645,  -0.0180, -0.2301,
              -0.0656, -0.0476, 0.1645,  -0.0231, -0.0489, -0.3512, -0.3281,
              -0.3023, 0.3288,  0.2983,  0.5140,  0.1852,  0.2157,  0.3464,
              0.4814,  0.5590,  0.2126,  0.0776,  0.3464,  0.4814,  0.5590,
              0.2126,  0.0776,  0.3464,  0.4814,  0.5590,  0.2126,  0.0776,
              0.0176,  0.1831,  0.0450,  0.0274,  -0.1381, 0.1249,  0.1286,
              0.4488,  0.3239,  0.3202,  0.1812,  0.3199,  0.5591,  0.3779,
              0.2392,  0.1812,  0.3199,  0.5591,  0.3779,  0.2392,  0.1812,
              0.3199,  0.5591,  0.3779,  0.2392,  0.0563,  0.1913,  0.1103,
              0.0540,  -0.0810, 0.1094,  0.0950,  0.4882,  0.3788,  0.3932,
              0.0897,  0.0391,  0.3949,  0.3052,  0.3558,  0.0897,  0.0391,
              0.3949,  0.3052,  0.3558,  0.0897,  0.0391,  0.3949,  0.3052,
              0.3558,  -0.0197, -0.0559, -0.0933, -0.0736, -0.0374, -0.1688,
              0.0694,  0.1632,  0.3320,  0.0938,  -0.3374, 0.1136,  0.2484,
              0.5858,  0.1348,  -0.3374, 0.1136,  0.2484,  0.5858,  0.1348,
              -0.3374, 0.1136,  0.2484,  0.5858,  0.1348,  -0.1686, 0.0442,
              0.0852,  0.2538,  0.0410,  0.0087,  0.1637,  -0.1951, -0.2038,
              -0.3588, 0.1702,  0.2275,  -0.0106, -0.1808, -0.2381, 0.1702,
              0.2275,  -0.0106, -0.1808, -0.2381, 0.1702,  0.2275,  -0.0106,
              -0.1808, -0.2381, 0.1615,  0.0638,  0.1845,  0.0230,  0.1207,
              0.0998,  0.2311,  0.3576,  0.2578,  0.1265,  0.2613,  -0.0654,
              -0.2308, -0.4921, -0.1654, 0.2613,  -0.0654, -0.2308, -0.4921,
              -0.1654, 0.2613,  -0.0654, -0.2308, -0.4921, -0.1654, 0.1615,
              -0.2965, -0.5884, -0.7499, -0.2919, -0.2800, -0.4527, -0.2751,
              0.0049,  0.1776,  -0.1522, -0.1356, 0.0023,  0.1545,  0.1379,
              -0.1522, -0.1356, 0.0023,  0.1545,  0.1379,  -0.1522, -0.1356,
              0.0023,  0.1545,  0.1379,  0.1278,  0.3171,  0.2774,  0.1496,
              -0.0397}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardWithPadding_2_2) {

  // std::cout << debug::print_backward_graph(result) << std::endl;
  conv_.debug_set_padding({2, 2});
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 50), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 112), 1e-6, 1e-4));

  // input_.grad()->print();
  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {-0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, 0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656, -0.0656,
              -0.0656, 0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5590,
              0.5590,  0.5590,  0.5590,  0.5590,  0.5590,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,  0.5591,
              0.5591,  0.5591,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,
              0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.3949,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,  0.2484,
              0.2484,  0.2484,  0.2484,  -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106, -0.0106,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308, -0.2308,
              -0.2308, -0.2308, -0.2308, -0.2308, 0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,  0.0023,
              0.0023}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardFilerWithStrides_2_2) {

  conv_.debug_set_stride({2, 2});
  run_forward_backward();

  // conv_.weights().print_shape();
  // conv_.weights().grad()->print_shape();
  // conv_.weights().grad()->print();
  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 8), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 8), 1e-6, 1e-4));

  // input_.grad()->print_shape();
  // input_.grad()->print();
  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.0051,  -0.1863, 0.4719,  -0.1863, 0.4668,  -0.0231, -0.0258,
              -0.3254, -0.0258, -0.3023, 0.0051,  -0.1863, 0.4719,  -0.1863,
              0.4668,  -0.0231, -0.0258, -0.3254, -0.0258, -0.3023, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.3288,  -0.0305, 0.5445,
              -0.0305, 0.2157,  0.0176,  0.1655,  -0.1205, 0.1655,  -0.1381,
              0.3288,  -0.0305, 0.5445,  -0.0305, 0.2157,  0.0176,  0.1655,
              -0.1205, 0.1655,  -0.1381, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.1249,  0.0037,  0.4451,  0.0037,  0.3202,  0.0563,
              0.1350,  -0.0247, 0.1350,  -0.0810, 0.1249,  0.0037,  0.4451,
              0.0037,  0.3202,  0.0563,  0.1350,  -0.0247, 0.1350,  -0.0810,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1094,  -0.0144,
              0.5026,  -0.0144, 0.3932,  -0.0197, -0.0362, -0.0571, -0.0362,
              -0.0374, 0.1094,  -0.0144, 0.5026,  -0.0144, 0.3932,  -0.0197,
              -0.0362, -0.0571, -0.0362, -0.0374, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  -0.1688, 0.2382,  -0.0750, 0.2382,  0.0938,
              -0.1686, 0.2128,  -0.1276, 0.2128,  0.0410,  -0.1688, 0.2382,
              -0.0750, 0.2382,  0.0938,  -0.1686, 0.2128,  -0.1276, 0.2128,
              0.0410,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0087,
              0.1550,  -0.3501, 0.1550,  -0.3588, 0.1615,  -0.0977, 0.2822,
              -0.0977, 0.1207,  0.0087,  0.1550,  -0.3501, 0.1550,  -0.3588,
              0.1615,  -0.0977, 0.2822,  -0.0977, 0.1207,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0998,  0.1313,  0.2263,  0.1313,
              0.1265,  0.1615,  -0.4580, -0.1304, -0.4580, -0.2919, 0.0998,
              0.1313,  0.2263,  0.1313,  0.1265,  0.1615,  -0.4580, -0.1304,
              -0.4580, -0.2919, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              -0.2800, -0.1727, -0.1024, -0.1727, 0.1776,  0.1278,  0.1893,
              0.0881,  0.1893,  -0.0397, -0.2800, -0.1727, -0.1024, -0.1727,
              0.1776,  0.1278,  0.1893,  0.0881,  0.1893,  -0.0397, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  -0.1863, 0.4719,
              -0.1863, 0.4668,  -0.0231, -0.0258, -0.3254, -0.0258, -0.3023,
              0.0051,  -0.1863, 0.4719,  -0.1863, 0.4668,  -0.0231, -0.0258,
              -0.3254, -0.0258, -0.3023, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.3288,  -0.0305, 0.5445,  -0.0305, 0.2157,  0.0176,
              0.1655,  -0.1205, 0.1655,  -0.1381, 0.3288,  -0.0305, 0.5445,
              -0.0305, 0.2157,  0.0176,  0.1655,  -0.1205, 0.1655,  -0.1381,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1249,  0.0037,
              0.4451,  0.0037,  0.3202,  0.0563,  0.1350,  -0.0247, 0.1350,
              -0.0810, 0.1249,  0.0037,  0.4451,  0.0037,  0.3202,  0.0563,
              0.1350,  -0.0247, 0.1350,  -0.0810, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.1094,  -0.0144, 0.5026,  -0.0144, 0.3932,
              -0.0197, -0.0362, -0.0571, -0.0362, -0.0374, 0.1094,  -0.0144,
              0.5026,  -0.0144, 0.3932,  -0.0197, -0.0362, -0.0571, -0.0362,
              -0.0374, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  -0.1688,
              0.2382,  -0.0750, 0.2382,  0.0938,  -0.1686, 0.2128,  -0.1276,
              0.2128,  0.0410,  -0.1688, 0.2382,  -0.0750, 0.2382,  0.0938,
              -0.1686, 0.2128,  -0.1276, 0.2128,  0.0410,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0087,  0.1550,  -0.3501, 0.1550,
              -0.3588, 0.1615,  -0.0977, 0.2822,  -0.0977, 0.1207,  0.0087,
              0.1550,  -0.3501, 0.1550,  -0.3588, 0.1615,  -0.0977, 0.2822,
              -0.0977, 0.1207,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0998,  0.1313,  0.2263,  0.1313,  0.1265,  0.1615,  -0.4580,
              -0.1304, -0.4580, -0.2919, 0.0998,  0.1313,  0.2263,  0.1313,
              0.1265,  0.1615,  -0.4580, -0.1304, -0.4580, -0.2919, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  -0.2800, -0.1727, -0.1024,
              -0.1727, 0.1776,  0.1278,  0.1893,  0.0881,  0.1893,  -0.0397,
              -0.2800, -0.1727, -0.1024, -0.1727, 0.1776,  0.1278,  0.1893,
              0.0881,  0.1893,  -0.0397, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardFilerWithStrides_2_3) {

  conv_.debug_set_stride({2, 3});
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 4), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 4), 1e-6, 1e-4));

  // input_.grad()->print();
  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.0051,  -0.1863, 0.4668,  0.0000,  0.0000,  -0.0231, -0.0258,
              -0.3023, 0.0000,  0.0000,  0.0051,  -0.1863, 0.4668,  0.0000,
              0.0000,  -0.0231, -0.0258, -0.3023, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.3288,  -0.0305, 0.2157,
              0.0000,  0.0000,  0.0176,  0.1655,  -0.1381, 0.0000,  0.0000,
              0.3288,  -0.0305, 0.2157,  0.0000,  0.0000,  0.0176,  0.1655,
              -0.1381, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.1249,  0.0037,  0.3202,  0.0000,  0.0000,  0.0563,
              0.1350,  -0.0810, 0.0000,  0.0000,  0.1249,  0.0037,  0.3202,
              0.0000,  0.0000,  0.0563,  0.1350,  -0.0810, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1094,  -0.0144,
              0.3932,  0.0000,  0.0000,  -0.0197, -0.0362, -0.0374, 0.0000,
              0.0000,  0.1094,  -0.0144, 0.3932,  0.0000,  0.0000,  -0.0197,
              -0.0362, -0.0374, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  -0.1688, 0.2382,  0.0938,  0.0000,  0.0000,
              -0.1686, 0.2128,  0.0410,  0.0000,  0.0000,  -0.1688, 0.2382,
              0.0938,  0.0000,  0.0000,  -0.1686, 0.2128,  0.0410,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0087,
              0.1550,  -0.3588, 0.0000,  0.0000,  0.1615,  -0.0977, 0.1207,
              0.0000,  0.0000,  0.0087,  0.1550,  -0.3588, 0.0000,  0.0000,
              0.1615,  -0.0977, 0.1207,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0998,  0.1313,  0.1265,  0.0000,
              0.0000,  0.1615,  -0.4580, -0.2919, 0.0000,  0.0000,  0.0998,
              0.1313,  0.1265,  0.0000,  0.0000,  0.1615,  -0.4580, -0.2919,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              -0.2800, -0.1727, 0.1776,  0.0000,  0.0000,  0.1278,  0.1893,
              -0.0397, 0.0000,  0.0000,  -0.2800, -0.1727, 0.1776,  0.0000,
              0.0000,  0.1278,  0.1893,  -0.0397, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  -0.1863, 0.4668,
              0.0000,  0.0000,  -0.0231, -0.0258, -0.3023, 0.0000,  0.0000,
              0.0051,  -0.1863, 0.4668,  0.0000,  0.0000,  -0.0231, -0.0258,
              -0.3023, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.3288,  -0.0305, 0.2157,  0.0000,  0.0000,  0.0176,
              0.1655,  -0.1381, 0.0000,  0.0000,  0.3288,  -0.0305, 0.2157,
              0.0000,  0.0000,  0.0176,  0.1655,  -0.1381, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1249,  0.0037,
              0.3202,  0.0000,  0.0000,  0.0563,  0.1350,  -0.0810, 0.0000,
              0.0000,  0.1249,  0.0037,  0.3202,  0.0000,  0.0000,  0.0563,
              0.1350,  -0.0810, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.1094,  -0.0144, 0.3932,  0.0000,  0.0000,
              -0.0197, -0.0362, -0.0374, 0.0000,  0.0000,  0.1094,  -0.0144,
              0.3932,  0.0000,  0.0000,  -0.0197, -0.0362, -0.0374, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  -0.1688,
              0.2382,  0.0938,  0.0000,  0.0000,  -0.1686, 0.2128,  0.0410,
              0.0000,  0.0000,  -0.1688, 0.2382,  0.0938,  0.0000,  0.0000,
              -0.1686, 0.2128,  0.0410,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0087,  0.1550,  -0.3588, 0.0000,
              0.0000,  0.1615,  -0.0977, 0.1207,  0.0000,  0.0000,  0.0087,
              0.1550,  -0.3588, 0.0000,  0.0000,  0.1615,  -0.0977, 0.1207,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0998,  0.1313,  0.1265,  0.0000,  0.0000,  0.1615,  -0.4580,
              -0.2919, 0.0000,  0.0000,  0.0998,  0.1313,  0.1265,  0.0000,
              0.0000,  0.1615,  -0.4580, -0.2919, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  -0.2800, -0.1727, 0.1776,
              0.0000,  0.0000,  0.1278,  0.1893,  -0.0397, 0.0000,  0.0000,
              -0.2800, -0.1727, 0.1776,  0.0000,  0.0000,  0.1278,  0.1893,
              -0.0397, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardFilerWithStrides_3_2) {

  conv_.debug_set_stride({3, 2});
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 8), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 8), 1e-6, 1e-4));

  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.0051,  -0.1863, 0.4719,  -0.1863, 0.4668,  -0.0231, -0.0258,
              -0.3254, -0.0258, -0.3023, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0051,  -0.1863, 0.4719,  -0.1863, 0.4668,  -0.0231,
              -0.0258, -0.3254, -0.0258, -0.3023, 0.3288,  -0.0305, 0.5445,
              -0.0305, 0.2157,  0.0176,  0.1655,  -0.1205, 0.1655,  -0.1381,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3288,  -0.0305,
              0.5445,  -0.0305, 0.2157,  0.0176,  0.1655,  -0.1205, 0.1655,
              -0.1381, 0.1249,  0.0037,  0.4451,  0.0037,  0.3202,  0.0563,
              0.1350,  -0.0247, 0.1350,  -0.0810, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.1249,  0.0037,  0.4451,  0.0037,  0.3202,
              0.0563,  0.1350,  -0.0247, 0.1350,  -0.0810, 0.1094,  -0.0144,
              0.5026,  -0.0144, 0.3932,  -0.0197, -0.0362, -0.0571, -0.0362,
              -0.0374, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1094,
              -0.0144, 0.5026,  -0.0144, 0.3932,  -0.0197, -0.0362, -0.0571,
              -0.0362, -0.0374, -0.1688, 0.2382,  -0.0750, 0.2382,  0.0938,
              -0.1686, 0.2128,  -0.1276, 0.2128,  0.0410,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  -0.1688, 0.2382,  -0.0750, 0.2382,
              0.0938,  -0.1686, 0.2128,  -0.1276, 0.2128,  0.0410,  0.0087,
              0.1550,  -0.3501, 0.1550,  -0.3588, 0.1615,  -0.0977, 0.2822,
              -0.0977, 0.1207,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0087,  0.1550,  -0.3501, 0.1550,  -0.3588, 0.1615,  -0.0977,
              0.2822,  -0.0977, 0.1207,  0.0998,  0.1313,  0.2263,  0.1313,
              0.1265,  0.1615,  -0.4580, -0.1304, -0.4580, -0.2919, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0998,  0.1313,  0.2263,
              0.1313,  0.1265,  0.1615,  -0.4580, -0.1304, -0.4580, -0.2919,
              -0.2800, -0.1727, -0.1024, -0.1727, 0.1776,  0.1278,  0.1893,
              0.0881,  0.1893,  -0.0397, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  -0.2800, -0.1727, -0.1024, -0.1727, 0.1776,  0.1278,
              0.1893,  0.0881,  0.1893,  -0.0397, 0.0051,  -0.1863, 0.4719,
              -0.1863, 0.4668,  -0.0231, -0.0258, -0.3254, -0.0258, -0.3023,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  -0.1863,
              0.4719,  -0.1863, 0.4668,  -0.0231, -0.0258, -0.3254, -0.0258,
              -0.3023, 0.3288,  -0.0305, 0.5445,  -0.0305, 0.2157,  0.0176,
              0.1655,  -0.1205, 0.1655,  -0.1381, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.3288,  -0.0305, 0.5445,  -0.0305, 0.2157,
              0.0176,  0.1655,  -0.1205, 0.1655,  -0.1381, 0.1249,  0.0037,
              0.4451,  0.0037,  0.3202,  0.0563,  0.1350,  -0.0247, 0.1350,
              -0.0810, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1249,
              0.0037,  0.4451,  0.0037,  0.3202,  0.0563,  0.1350,  -0.0247,
              0.1350,  -0.0810, 0.1094,  -0.0144, 0.5026,  -0.0144, 0.3932,
              -0.0197, -0.0362, -0.0571, -0.0362, -0.0374, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.1094,  -0.0144, 0.5026,  -0.0144,
              0.3932,  -0.0197, -0.0362, -0.0571, -0.0362, -0.0374, -0.1688,
              0.2382,  -0.0750, 0.2382,  0.0938,  -0.1686, 0.2128,  -0.1276,
              0.2128,  0.0410,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              -0.1688, 0.2382,  -0.0750, 0.2382,  0.0938,  -0.1686, 0.2128,
              -0.1276, 0.2128,  0.0410,  0.0087,  0.1550,  -0.3501, 0.1550,
              -0.3588, 0.1615,  -0.0977, 0.2822,  -0.0977, 0.1207,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0087,  0.1550,  -0.3501,
              0.1550,  -0.3588, 0.1615,  -0.0977, 0.2822,  -0.0977, 0.1207,
              0.0998,  0.1313,  0.2263,  0.1313,  0.1265,  0.1615,  -0.4580,
              -0.1304, -0.4580, -0.2919, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0998,  0.1313,  0.2263,  0.1313,  0.1265,  0.1615,
              -0.4580, -0.1304, -0.4580, -0.2919, -0.2800, -0.1727, -0.1024,
              -0.1727, 0.1776,  0.1278,  0.1893,  0.0881,  0.1893,  -0.0397,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  -0.2800, -0.1727,
              -0.1024, -0.1727, 0.1776,  0.1278,  0.1893,  0.0881,  0.1893,
              -0.0397}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardFilerWithStrides_3_3) {

  conv_.debug_set_stride({3, 3});
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor({out_channels, in_channels, kernel_h, kernel_w}, 4), 1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 4), 1e-6, 1e-4));

  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.0051,  -0.1863, 0.4668,  0.0000,  0.0000,  -0.0231, -0.0258,
              -0.3023, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0051,  -0.1863, 0.4668,  0.0000,  0.0000,  -0.0231,
              -0.0258, -0.3023, 0.0000,  0.0000,  0.3288,  -0.0305, 0.2157,
              0.0000,  0.0000,  0.0176,  0.1655,  -0.1381, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3288,  -0.0305,
              0.2157,  0.0000,  0.0000,  0.0176,  0.1655,  -0.1381, 0.0000,
              0.0000,  0.1249,  0.0037,  0.3202,  0.0000,  0.0000,  0.0563,
              0.1350,  -0.0810, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.1249,  0.0037,  0.3202,  0.0000,  0.0000,
              0.0563,  0.1350,  -0.0810, 0.0000,  0.0000,  0.1094,  -0.0144,
              0.3932,  0.0000,  0.0000,  -0.0197, -0.0362, -0.0374, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1094,
              -0.0144, 0.3932,  0.0000,  0.0000,  -0.0197, -0.0362, -0.0374,
              0.0000,  0.0000,  -0.1688, 0.2382,  0.0938,  0.0000,  0.0000,
              -0.1686, 0.2128,  0.0410,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  -0.1688, 0.2382,  0.0938,  0.0000,
              0.0000,  -0.1686, 0.2128,  0.0410,  0.0000,  0.0000,  0.0087,
              0.1550,  -0.3588, 0.0000,  0.0000,  0.1615,  -0.0977, 0.1207,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0087,  0.1550,  -0.3588, 0.0000,  0.0000,  0.1615,  -0.0977,
              0.1207,  0.0000,  0.0000,  0.0998,  0.1313,  0.1265,  0.0000,
              0.0000,  0.1615,  -0.4580, -0.2919, 0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0998,  0.1313,  0.1265,
              0.0000,  0.0000,  0.1615,  -0.4580, -0.2919, 0.0000,  0.0000,
              -0.2800, -0.1727, 0.1776,  0.0000,  0.0000,  0.1278,  0.1893,
              -0.0397, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  -0.2800, -0.1727, 0.1776,  0.0000,  0.0000,  0.1278,
              0.1893,  -0.0397, 0.0000,  0.0000,  0.0051,  -0.1863, 0.4668,
              0.0000,  0.0000,  -0.0231, -0.0258, -0.3023, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  -0.1863,
              0.4668,  0.0000,  0.0000,  -0.0231, -0.0258, -0.3023, 0.0000,
              0.0000,  0.3288,  -0.0305, 0.2157,  0.0000,  0.0000,  0.0176,
              0.1655,  -0.1381, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.3288,  -0.0305, 0.2157,  0.0000,  0.0000,
              0.0176,  0.1655,  -0.1381, 0.0000,  0.0000,  0.1249,  0.0037,
              0.3202,  0.0000,  0.0000,  0.0563,  0.1350,  -0.0810, 0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1249,
              0.0037,  0.3202,  0.0000,  0.0000,  0.0563,  0.1350,  -0.0810,
              0.0000,  0.0000,  0.1094,  -0.0144, 0.3932,  0.0000,  0.0000,
              -0.0197, -0.0362, -0.0374, 0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.1094,  -0.0144, 0.3932,  0.0000,
              0.0000,  -0.0197, -0.0362, -0.0374, 0.0000,  0.0000,  -0.1688,
              0.2382,  0.0938,  0.0000,  0.0000,  -0.1686, 0.2128,  0.0410,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              -0.1688, 0.2382,  0.0938,  0.0000,  0.0000,  -0.1686, 0.2128,
              0.0410,  0.0000,  0.0000,  0.0087,  0.1550,  -0.3588, 0.0000,
              0.0000,  0.1615,  -0.0977, 0.1207,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0087,  0.1550,  -0.3588,
              0.0000,  0.0000,  0.1615,  -0.0977, 0.1207,  0.0000,  0.0000,
              0.0998,  0.1313,  0.1265,  0.0000,  0.0000,  0.1615,  -0.4580,
              -0.2919, 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
              0.0000,  0.0998,  0.1313,  0.1265,  0.0000,  0.0000,  0.1615,
              -0.4580, -0.2919, 0.0000,  0.0000,  -0.2800, -0.1727, 0.1776,
              0.0000,  0.0000,  0.1278,  0.1893,  -0.0397, 0.0000,  0.0000,
              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  -0.2800, -0.1727,
              0.1776,  0.0000,  0.0000,  0.1278,  0.1893,  -0.0397, 0.0000,
              0.0000}),
      1e-6, 1e-3));
}

TEST_F(Conv2dBackwardTest, BackwardWithPadding_2_2_Strides_2_3) {

  conv_.debug_set_padding({2, 2});
  conv_.debug_set_stride({2, 3});
  run_forward_backward();

  EXPECT_TRUE(conv_.weights().grad()->strict_allclose(
      Tensor(
          {out_channels, in_channels, kernel_h, kernel_w},
          {12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,
           12., 6., 12., 8.,  4., 8.,  12., 6., 12., 8.,  4., 8.,  12., 6., 12.,
           8.,  4., 8.}),
      1e-6, 1e-4));

  EXPECT_TRUE(conv_.bias().grad()->strict_allclose(
      Tensor({out_channels, 1, 1}, 24), 1e-6, 1e-4));

  // input_.grad()->print();
  EXPECT_TRUE(input_.grad()->strict_allclose(
      Tensor(input_.shape(),
             {0.4668,  0.0051,  -0.1863, 0.4668,  0.0051,  -0.3023, -0.0231,
              -0.0258, -0.3023, -0.0231, 0.4668,  0.0051,  -0.1863, 0.4668,
              0.0051,  -0.3023, -0.0231, -0.0258, -0.3023, -0.0231, 0.4668,
              0.0051,  -0.1863, 0.4668,  0.0051,  0.2157,  0.3288,  -0.0305,
              0.2157,  0.3288,  -0.1381, 0.0176,  0.1655,  -0.1381, 0.0176,
              0.2157,  0.3288,  -0.0305, 0.2157,  0.3288,  -0.1381, 0.0176,
              0.1655,  -0.1381, 0.0176,  0.2157,  0.3288,  -0.0305, 0.2157,
              0.3288,  0.3202,  0.1249,  0.0037,  0.3202,  0.1249,  -0.0810,
              0.0563,  0.1350,  -0.0810, 0.0563,  0.3202,  0.1249,  0.0037,
              0.3202,  0.1249,  -0.0810, 0.0563,  0.1350,  -0.0810, 0.0563,
              0.3202,  0.1249,  0.0037,  0.3202,  0.1249,  0.3932,  0.1094,
              -0.0144, 0.3932,  0.1094,  -0.0374, -0.0197, -0.0362, -0.0374,
              -0.0197, 0.3932,  0.1094,  -0.0144, 0.3932,  0.1094,  -0.0374,
              -0.0197, -0.0362, -0.0374, -0.0197, 0.3932,  0.1094,  -0.0144,
              0.3932,  0.1094,  0.0938,  -0.1688, 0.2382,  0.0938,  -0.1688,
              0.0410,  -0.1686, 0.2128,  0.0410,  -0.1686, 0.0938,  -0.1688,
              0.2382,  0.0938,  -0.1688, 0.0410,  -0.1686, 0.2128,  0.0410,
              -0.1686, 0.0938,  -0.1688, 0.2382,  0.0938,  -0.1688, -0.3588,
              0.0087,  0.1550,  -0.3588, 0.0087,  0.1207,  0.1615,  -0.0977,
              0.1207,  0.1615,  -0.3588, 0.0087,  0.1550,  -0.3588, 0.0087,
              0.1207,  0.1615,  -0.0977, 0.1207,  0.1615,  -0.3588, 0.0087,
              0.1550,  -0.3588, 0.0087,  0.1265,  0.0998,  0.1313,  0.1265,
              0.0998,  -0.2919, 0.1615,  -0.4580, -0.2919, 0.1615,  0.1265,
              0.0998,  0.1313,  0.1265,  0.0998,  -0.2919, 0.1615,  -0.4580,
              -0.2919, 0.1615,  0.1265,  0.0998,  0.1313,  0.1265,  0.0998,
              0.1776,  -0.2800, -0.1727, 0.1776,  -0.2800, -0.0397, 0.1278,
              0.1893,  -0.0397, 0.1278,  0.1776,  -0.2800, -0.1727, 0.1776,
              -0.2800, -0.0397, 0.1278,  0.1893,  -0.0397, 0.1278,  0.1776,
              -0.2800, -0.1727, 0.1776,  -0.2800, 0.4668,  0.0051,  -0.1863,
              0.4668,  0.0051,  -0.3023, -0.0231, -0.0258, -0.3023, -0.0231,
              0.4668,  0.0051,  -0.1863, 0.4668,  0.0051,  -0.3023, -0.0231,
              -0.0258, -0.3023, -0.0231, 0.4668,  0.0051,  -0.1863, 0.4668,
              0.0051,  0.2157,  0.3288,  -0.0305, 0.2157,  0.3288,  -0.1381,
              0.0176,  0.1655,  -0.1381, 0.0176,  0.2157,  0.3288,  -0.0305,
              0.2157,  0.3288,  -0.1381, 0.0176,  0.1655,  -0.1381, 0.0176,
              0.2157,  0.3288,  -0.0305, 0.2157,  0.3288,  0.3202,  0.1249,
              0.0037,  0.3202,  0.1249,  -0.0810, 0.0563,  0.1350,  -0.0810,
              0.0563,  0.3202,  0.1249,  0.0037,  0.3202,  0.1249,  -0.0810,
              0.0563,  0.1350,  -0.0810, 0.0563,  0.3202,  0.1249,  0.0037,
              0.3202,  0.1249,  0.3932,  0.1094,  -0.0144, 0.3932,  0.1094,
              -0.0374, -0.0197, -0.0362, -0.0374, -0.0197, 0.3932,  0.1094,
              -0.0144, 0.3932,  0.1094,  -0.0374, -0.0197, -0.0362, -0.0374,
              -0.0197, 0.3932,  0.1094,  -0.0144, 0.3932,  0.1094,  0.0938,
              -0.1688, 0.2382,  0.0938,  -0.1688, 0.0410,  -0.1686, 0.2128,
              0.0410,  -0.1686, 0.0938,  -0.1688, 0.2382,  0.0938,  -0.1688,
              0.0410,  -0.1686, 0.2128,  0.0410,  -0.1686, 0.0938,  -0.1688,
              0.2382,  0.0938,  -0.1688, -0.3588, 0.0087,  0.1550,  -0.3588,
              0.0087,  0.1207,  0.1615,  -0.0977, 0.1207,  0.1615,  -0.3588,
              0.0087,  0.1550,  -0.3588, 0.0087,  0.1207,  0.1615,  -0.0977,
              0.1207,  0.1615,  -0.3588, 0.0087,  0.1550,  -0.3588, 0.0087,
              0.1265,  0.0998,  0.1313,  0.1265,  0.0998,  -0.2919, 0.1615,
              -0.4580, -0.2919, 0.1615,  0.1265,  0.0998,  0.1313,  0.1265,
              0.0998,  -0.2919, 0.1615,  -0.4580, -0.2919, 0.1615,  0.1265,
              0.0998,  0.1313,  0.1265,  0.0998,  0.1776,  -0.2800, -0.1727,
              0.1776,  -0.2800, -0.0397, 0.1278,  0.1893,  -0.0397, 0.1278,
              0.1776,  -0.2800, -0.1727, 0.1776,  -0.2800, -0.0397, 0.1278,
              0.1893,  -0.0397, 0.1278,  0.1776,  -0.2800, -0.1727, 0.1776,
              -0.2800}),
      1e-6, 1e-3));
}
